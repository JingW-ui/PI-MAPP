import pypinyin
from DrissionPage import ChromiumPage
import pandas as pd
import time
import re
from pypinyin import lazy_pinyin
# åŸºç¡€URL
BASE_URL = "https://www.cdgdc.edu.cn"
START_URL = f"{BASE_URL}/dslxkpgjggb/"
# -------------------- 1. é—¨ç±»â†’ç¼©å†™/ä»£ç æ®µ æ˜ å°„è¡¨ --------------------
CATEGORY_MAP = {
    "äººæ–‡ç¤¾ç§‘ç±»": {"abbr": "rwskl", "code_range": range(  1,   7)},   # 01xx
    "ç†å­¦":       {"abbr": "lx",    "code_range": range(  7,  10)},   # 07xx
    "å·¥å­¦":       {"abbr": "gx",    "code_range": range( 10,  14)},   # 08xx
    "å†œå­¦":       {"abbr": "nx",    "code_range": range( 14,  18)},   # 09xx
    "åŒ»å­¦":       {"abbr": "yx",    "code_range": range( 18,  22)},   # 10xx
    "ç®¡ç†å­¦":     {"abbr": "glx",   "code_range": range( 22,  26)},   # 12xx
    "è‰ºæœ¯å­¦":     {"abbr": "ysx",   "code_range": range( 26,  30)},   # 13xx
}
# ---------- æ‹¼éŸ³ç¼©å†™ -> ä¸­æ–‡ ----------
PIN2CN = {
    'rwskl': 'äººæ–‡ç¤¾ç§‘ç±»',
    'lx':    'ç†å­¦',
    'gx':    'å·¥å­¦',
    'nx':    'å†œå­¦',
    'yx':    'åŒ»å­¦',
    'glx':   'ç®¡ç†å­¦',
    'ysx':   'è‰ºæœ¯å­¦',
}

# ä¸­æ–‡ -> æ‹¼éŸ³ç¼©å†™
CN2PIN = {v: k for k, v in PIN2CN.items()}

# ä¸¤ä¸ªå·¥å…·å‡½æ•°
def get_cn(pinyin_abbr: str) -> str:
    """è¾“å…¥ 'rwskl'  -> è¾“å‡º 'äººæ–‡ç¤¾ç§‘ç±»'"""
    return PIN2CN.get(pinyin_abbr.strip().lower(), 'æœªçŸ¥é—¨ç±»')

def get_pinyin(cn_name: str) -> str:
    """è¾“å…¥ 'äººæ–‡ç¤¾ç§‘ç±»' -> è¾“å‡º 'rwskl'"""
    return CN2PIN.get(cn_name.strip(), 'unknown')
# -------------- 2. ä»æ–‡æœ¬ä¸­æå–å­¦ç§‘ä»£ç +åç§° --------------
SUBJ_RE = re.compile(r"^\s*(\d{4})\s+([^\s].+)$", re.M)

def extract_subjects_from_text(text: str):
    """
    ä» print_all_subjects æ‰“å°å‡ºçš„åŸå§‹æ–‡æœ¬é‡Œï¼Œ
    æå–æ‰€æœ‰ 4 ä½å­¦ç§‘ä»£ç åŠåç§°
    è¿”å› list[ (code, name) ]
    """
    return SUBJ_RE.findall(text)


# -------------- 4. æ„é€ å•ä¸ªå­¦ç§‘ URL --------------
def build_subject_url(code: str, name: str,abbr: str) -> str:
    """
    æ„é€ è§„åˆ™ï¼š
    https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/{é—¨ç±»ç¼©å†™}/a{ä»£ç }_{é¦–å­—æ¯å°å†™æ‹¼éŸ³}.htm
    """
    chars = re.findall(r'[\u4e00-\u9fff]', name)
    # é¦–å­—æ¯å°å†™
    py_part = ''.join(lazy_pinyin(chars, style=pypinyin.FIRST_LETTER)).lower()

    return f"https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/{abbr}/a{code}_{py_part}.htm"
# -------------- 5. ä¸€é”®ç”Ÿæˆæ‰€æœ‰å­¦ç§‘ URL --------------
def build_all_subject_urls(text: str,abbr: str):
    """
    è¾“å…¥ï¼šprint_all_subjects å¾—åˆ°çš„åŸå§‹æ–‡æœ¬
    """
    subs = extract_subjects_from_text(text)
    url_list = []
    for code, name in subs:
        url = build_subject_url(code, name, abbr)
        url_list.append({
                'category': get_cn(abbr),
                'code': code,
                'name': name,
                'url': url
            })
    return url_list

def setup_browser():
    """è®¾ç½®æµè§ˆå™¨é…ç½®"""
    page = ChromiumPage()
    page.set.window.size(1200, 800)
    return page


def find_fourth_round_link(page):
    """åœ¨ä¸»é¡µæ‰¾åˆ°ç¬¬å››è½®å­¦ç§‘è¯„ä¼°çš„é“¾æ¥"""
    print("æ­£åœ¨è®¿é—®ä¸»é¡µé¢...")
    page.get(START_URL)
    time.sleep(2)

    # æŸ¥æ‰¾åŒ…å«"ç¬¬å››è½®"å’Œ"å­¦ç§‘è¯„ä¼°"çš„é“¾æ¥
    links = page.eles('tag:a')
    for link in links:
        link_text = link.text.strip()
        if 'a0' in link_text :
            fourth_round_url = link.attr('href')
            if fourth_round_url.startswith('/'):
                fourth_round_url = BASE_URL + fourth_round_url
            print(f"æ‰¾åˆ°ç¬¬å››è½®è¯„ä¼°é“¾æ¥: {fourth_round_url}")
            return fourth_round_url

    print("æœªæ‰¾åˆ°ç¬¬å››è½®è¯„ä¼°é“¾æ¥")
    return None


def get_all_subject_links(page, fourth_round_url):
    """è·å–æ‰€æœ‰å­¦ç§‘çš„é“¾æ¥"""
    print("æ­£åœ¨è®¿é—®ç¬¬å››è½®è¯„ä¼°ä¸»é¡µï¼Œè·å–æ‰€æœ‰å­¦ç§‘é“¾æ¥...")
    page.get(fourth_round_url)
    time.sleep(3)

    # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬å†…å®¹
    table = page.ele('tag:table')
    page_text = table.text

    # è§£æå­¦ç§‘é—¨ç±»å’Œå­¦ç§‘ä»£ç 
    subjects = []

    # å®šä¹‰å­¦ç§‘é—¨ç±»
    categories = ['äººæ–‡ç¤¾ç§‘ç±»', 'ç†å­¦', 'å·¥å­¦', 'å†œå­¦', 'åŒ»å­¦', 'ç®¡ç†å­¦', 'è‰ºæœ¯å­¦']
    current_category = None

    lines = page_text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­¦ç§‘é—¨ç±»
        if any(category in line for category in categories):
            current_category = line
            print(f"æ‰¾åˆ°å­¦ç§‘é—¨ç±»: {current_category}")
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­¦ç§‘è¡Œï¼ˆåŒ…å«4ä½æ•°å­—ä»£ç ï¼‰
        subject_match = re.match(r'(\d{4})\s+(.+)', line)
        if subject_match and current_category:
            subject_code = subject_match.group(1)
            subject_name = subject_match.group(2).strip()

            # æ„å»ºå­¦ç§‘é“¾æ¥ï¼ˆæ ¹æ®æ‚¨æä¾›çš„æ¨¡å¼ï¼‰
            # ä¾‹å¦‚ï¼ša0819_kygc.htm
            subject_url = f"https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/gx/a{subject_code}_{get_pinyin_abbr(subject_name)}.htm"

            subjects.append({
                'category': current_category,
                'code': subject_code,
                'name': subject_name,
                'url': subject_url
            })
            print(f"  æ‰¾åˆ°å­¦ç§‘: {subject_code} {subject_name}")

    print(f"æ€»å…±æ‰¾åˆ° {len(subjects)} ä¸ªå­¦ç§‘")
    return subjects


def get_pinyin_abbr(chinese_name):
    """ç”Ÿæˆå­¦ç§‘åç§°çš„æ‹¼éŸ³ç¼©å†™ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰"""
    # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„æ˜ å°„ï¼Œå®é™…åº”è¯¥ä½¿ç”¨å®Œæ•´çš„æ‹¼éŸ³è½¬æ¢åº“
    pinyin_map = {
        'åŠ›å­¦': 'lx',
        'æœºæ¢°å·¥ç¨‹': 'jxgc',
        'å…‰å­¦å·¥ç¨‹': 'gxgc',
        'ä»ªå™¨ç§‘å­¦ä¸æŠ€æœ¯': 'yqkx',
        'ææ–™ç§‘å­¦ä¸å·¥ç¨‹': 'clkx',
        'å†¶é‡‘å·¥ç¨‹': 'yjgc',
        'åŠ¨åŠ›å·¥ç¨‹åŠå·¥ç¨‹çƒ­ç‰©ç†': 'dlgc',
        'ç”µæ°”å·¥ç¨‹': 'dqgc',
        'ç”µå­ç§‘å­¦ä¸æŠ€æœ¯': 'dzkx',
        'ä¿¡æ¯ä¸é€šä¿¡å·¥ç¨‹': 'xxtx',
        'æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹': 'kzkx',
        'è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯': 'jsjkx',
        'å»ºç­‘å­¦': 'jzx',
        'åœŸæœ¨å·¥ç¨‹': 'tmgc',
        'æ°´åˆ©å·¥ç¨‹': 'slgc',
        'æµ‹ç»˜ç§‘å­¦ä¸æŠ€æœ¯': 'chkx',
        'åŒ–å­¦å·¥ç¨‹ä¸æŠ€æœ¯': 'hxgc',
        'åœ°è´¨èµ„æºä¸åœ°è´¨å·¥ç¨‹': 'dzzy',
        'çŸ¿ä¸šå·¥ç¨‹': 'kygc',
        'çŸ³æ²¹ä¸å¤©ç„¶æ°”å·¥ç¨‹': 'sytrq',
        'çººç»‡ç§‘å­¦ä¸å·¥ç¨‹': 'fzkx',
        'è½»å·¥æŠ€æœ¯ä¸å·¥ç¨‹': 'qgjs',
        'äº¤é€šè¿è¾“å·¥ç¨‹': 'jtys',
        'èˆ¹èˆ¶ä¸æµ·æ´‹å·¥ç¨‹': 'cbhy',
        'èˆªç©ºå®‡èˆªç§‘å­¦ä¸æŠ€æœ¯': 'hkyh',
        'å…µå™¨ç§‘å­¦ä¸æŠ€æœ¯': 'bqkx',
        'æ ¸ç§‘å­¦ä¸æŠ€æœ¯': 'hkx',
        'å†œä¸šå·¥ç¨‹': 'nygc',
        'æ—ä¸šå·¥ç¨‹': 'lygc',
        'ç¯å¢ƒç§‘å­¦ä¸å·¥ç¨‹': 'hjkx',
        'ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹': 'swyx',
        'é£Ÿå“ç§‘å­¦ä¸å·¥ç¨‹': 'spkx',
        'åŸä¹¡è§„åˆ’å­¦': 'cxgh',
        'é£æ™¯å›­æ—å­¦': 'fjyl',
        'è½¯ä»¶å·¥ç¨‹': 'rjgc',
        'å®‰å…¨ç§‘å­¦ä¸å·¥ç¨‹': 'aqkx'
    }

    return pinyin_map.get(chinese_name, chinese_name[:4].lower())


def parse_subject_table(page, subject_url, category_name, subject_name, subject_code):
    """è§£æå…·ä½“å­¦ç§‘çš„è¯„ä¼°ç»“æœè¡¨æ ¼"""
    print(f"æ­£åœ¨è§£æ: {subject_code} {subject_name}")

    try:
        page.get(subject_url)
        time.sleep(2)

        # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬
        table = page.ele('tag:table')
        page_text = table.text

        # è§£æè¯„ä¼°ç»“æœ
        results = []
        lines = page_text.split('\n')

        current_grade = None
        for line in lines:
            line = line.strip()

            # æ£€æµ‹ç­‰çº§è¡Œï¼ˆA+, A, A-, B+, B, B-, C+, C, C-ï¼‰
            grade_match = re.match(r'([ABC][+-]?)', line)
            if grade_match:
                current_grade = grade_match.group(1)
                # continue

            # æ£€æµ‹åŒ…å«ç­‰çº§å’Œå­¦æ ¡ä¿¡æ¯çš„è¡Œ
            # åŒ¹é…æ ¼å¼: "A+    10290      ä¸­å›½çŸ¿ä¸šå¤§å­¦"
            # print(line)
            combined_match = re.match(r'^([ABC][+-]?)\s+(\d{5})\s+([\u4e00-\u9fff]+å¤§å­¦|[\u4e00-\u9fff]+å­¦é™¢)', line)
            if combined_match:
                grade = combined_match.group(1)
                school_code = combined_match.group(2)
                school_name = combined_match.group(3)

                results.append({
                    'å­¦ç§‘ä»£ç ': subject_code,
                    'å­¦ç§‘åç§°': subject_name,
                    'å­¦ç§‘é—¨ç±»': category_name,
                    'å­¦æ ¡ä»£ç ': school_code,
                    'å­¦æ ¡åç§°': school_name,
                    'è¯„ä¼°ç­‰çº§': grade
                })
                continue
            # æ£€æµ‹å­¦æ ¡è¡Œï¼ˆå­¦æ ¡ä»£ç  + å­¦æ ¡åç§°ï¼‰
            school_match = re.match(r'(\d{5})\s+([\u4e00-\u9fff]+å¤§å­¦|[\u4e00-\u9fff]+å­¦é™¢)', line)
            if school_match and current_grade:
                school_code = school_match.group(1)
                school_name = school_match.group(2)

                results.append({
                    'å­¦ç§‘ä»£ç ': subject_code,
                    'å­¦ç§‘åç§°': subject_name,
                    'å­¦ç§‘é—¨ç±»': category_name,
                    'å­¦æ ¡ä»£ç ': school_code,
                    'å­¦æ ¡åç§°': school_name,
                    'è¯„ä¼°ç­‰çº§': current_grade
                })

        if results:
            df = pd.DataFrame(results)
            print(f"  æˆåŠŸæå– {len(df)} æ¡è¯„ä¼°ç»“æœ")
            return df
        else:
            print(f"  æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ: {subject_name}")
            return None

    except Exception as e:
        print(f"  è§£æå¤±è´¥ {subject_name}: {e}")
        return None





def debug_single_page():
    """è°ƒè¯•å‡½æ•°ï¼šåªçˆ¬å–å•ä¸ªé¡µé¢è¿›è¡Œæµ‹è¯•"""
    page = setup_browser()

    # æµ‹è¯•å•ä¸ªå­¦ç§‘
    test_url = "https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/gx/a0819_kygc.htm"
    category_name = "å·¥å­¦"
    subject_name = "çŸ¿ä¸šå·¥ç¨‹"
    subject_code = "0819"

    try:
        result_df = parse_subject_table(page, test_url, category_name, subject_name, subject_code)
        if result_df is not None:
            print("æµ‹è¯•æˆåŠŸï¼æ•°æ®é¢„è§ˆ:")
            print(result_df)
            result_df.to_csv("debug_test.csv", index=False, encoding='utf-8-sig')

            # æ˜¾ç¤ºè¿™ä¸ªå­¦ç§‘çš„ç»Ÿè®¡
            print(f"\n{subject_name} è¯„ä¼°ç»“æœç»Ÿè®¡:")
            print(result_df['è¯„ä¼°ç­‰çº§'].value_counts().sort_index())
        else:
            print("æµ‹è¯•å¤±è´¥")
    finally:
        page.quit()


def get_text(page, url):

    try:
        page.get(url)
        time.sleep(2)

        # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬
        table = page.ele('tag:table')
        page_text = table.text
        return page_text


    except Exception as e:
        print(f"  è§£æå¤±è´¥ {url}: {e}")
        return None


def get_all_subjects():
    url_list = []
    url_list_subjects = []
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/rwskl/a0101_zx.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/lx/a0701_sx.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/gx/a0801_lx.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/nx/a0901_zwx.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/yx/a1001_jcyx.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/glx/a1201_glkxygc.htm')
    url_list.append('https://www.cdgdc.edu.cn/dslxkpgjggb/xkpm/ysx/a1301_ysxll.htm')
    for url in url_list:
        page = setup_browser()
        text = get_text(page, url)
        # print(text)
        abbr = url.split('/')[-2]
        df_url = build_all_subject_urls(text,abbr)
        for furl in df_url:
            url_list_subjects.append(furl)
    return  url_list_subjects
def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–æµè§ˆå™¨...")
    page = setup_browser()
    all_results = []

    try:

        # 2. è·å–æ‰€æœ‰å­¦ç§‘é“¾æ¥
        subjects = get_all_subjects()
        print(f"\nå¼€å§‹çˆ¬å– {len(subjects)} ä¸ªå­¦ç§‘çš„æ•°æ®...")

        # 3. éå†æ¯ä¸ªå­¦ç§‘
        for i, subject in enumerate(subjects, 1):
            print(f"\n[{i}/{len(subjects)}] ", end="")

            # è§£æå­¦ç§‘é¡µé¢
            result_df = parse_subject_table(
                page,
                subject['url'],
                subject['category'],
                subject['name'],
                subject['code']
            )

            if result_df is not None:
                all_results.append(result_df)

            # å‹å¥½å»¶è¿Ÿ
            time.sleep(1)

        # 4. ä¿å­˜ç»“æœ
        if all_results:
            final_df = pd.concat(all_results, ignore_index=True)

            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            output_file = "å­¦ç§‘è¯„ä¼°ç»“æœ_ç¬¬å››è½®.csv"
            final_df.to_csv(output_file, index=False, encoding='utf-8-sig')

            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“Š å…±è·å– {len(all_results)} ä¸ªå­¦ç§‘çš„æ•°æ®")
            print(f"ğŸ« å…± {len(final_df)} æ¡è¯„ä¼°è®°å½•")
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            print("\næ•°æ®é¢„è§ˆ:")
            print(final_df.head(10))

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print("\nè¯„ä¼°ç­‰çº§ç»Ÿè®¡:")
            print(final_df['è¯„ä¼°ç­‰çº§'].value_counts().sort_index())

        else:
            print("æœªè·å–åˆ°ä»»ä½•æ•°æ®")

    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    finally:
        print("\nå…³é—­æµè§ˆå™¨...")
        page.quit()
if __name__ == "__main__":
    # è¿è¡Œè°ƒè¯•æ¨¡å¼ï¼ˆæµ‹è¯•å•ä¸ªé¡µé¢ï¼‰
    # print("=== è°ƒè¯•æ¨¡å¼ ===")
    # debug_single_page()

    # è¿è¡Œå®Œæ•´çˆ¬å–ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œï¼‰
    print("=== å®Œæ•´çˆ¬å–æ¨¡å¼ ===")
    main()
